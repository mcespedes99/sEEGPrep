#---- begin snakebids boilerplate ----------------------------------------------

import snakebids
from snakebids import bids
from os.path import join
import pandas as pd
import numpy as np
import copy
import os

configfile: workflow.source_path('../config/snakebids.yml')

# Get input wildcards
inputs = snakebids.generate_inputs(
    bids_dir=config["bids_dir"],
    pybids_inputs=config["pybids_inputs"],
    pybids_database_dir=config.get("pybids_db_dir"),
    pybids_reset_database=config.get("pybids_db_reset"),
    derivatives=config.get("derivatives", None),
    participant_label=config.get("participant_label", None),
    exclude_participant_label=config.get("exclude_participant_label", None),
    use_bids_inputs=True,
)

# Filter inputs based on annotations
def get_inputs(input_path, **zip_lists):
    # Possible edf files
    annot_files = expand(
                input_path,
                zip,
                **zip_lists
            )
    # print(annot_files)
    # Convert zip list to tuples and add to a set
    set_wildcards_vals = set()
    list_wildcards_vals = []
    values_wildcards = list(zip_lists.values())
    for idx in range(len(annot_files)):
        tmp_tuple = tuple([value[idx] for value in values_wildcards])
        set_wildcards_vals.add(tmp_tuple)
        # Also save in list to use it in the filtering (an ordered list is needed)
        list_wildcards_vals.append(tmp_tuple)
    
    # Save number of clips per session/subject
    input_keys = list(zip_lists.keys())
    subj_id = input_keys.index('subject')
    ses_id = input_keys.index('session')
    number_clips = {}
    # Create filter list
    out_wildcards_vals = []
    # Check annotation files to filter inputs
    for idx, annot_file in enumerate(annot_files):
        # Open annotation file to check if any event is in the data
        annot = pd.read_csv(annot_file, sep='\t')
        # Find time stamps indexes where the 'awake trigger' event is happening
        # returns True on all the indexes where the event is found
        instances = annot['event'].str.match('awake trigger', case=False, na=False)
        # Add specific amount of wildcards depending on the number of instances found
        n = np.count_nonzero(instances)
        out_wildcards_vals += [list_wildcards_vals[idx] + tuple([f'{number:02}']) for number in np.arange(1,n+1)]
        # Save number of clips
        subj_identifier = f'subj_{list_wildcards_vals[idx][subj_id]}'
        ses_identifier = f'ses_{list_wildcards_vals[idx][ses_id]}'
        if subj_identifier in number_clips:
            if ses_identifier in number_clips[subj_identifier]:
                number_clips[subj_identifier][ses_identifier] = max(number_clips[subj_identifier][ses_identifier], n)
            else:
                number_clips[subj_identifier][ses_identifier] = n
        else:
            number_clips[subj_identifier] = {ses_identifier: n}
    
    # First get dict keys
    out_ziplist_keys = list(zip_lists.keys()) +['clip']
    # Convert the list of tuples of wc combinations to list of wc lists (separated)
    out_ziplist_vals = []
    for key_id in range(len(out_ziplist_keys)):
        out_ziplist_vals.append([tuple_val[key_id] for tuple_val in out_wildcards_vals])
    # Construct dictionary
    out_ziplist = dict(zip(out_ziplist_keys, out_ziplist_vals))
    return out_ziplist, number_clips

#this adds constraints to the bids naming
wildcard_constraints:  **snakebids.get_wildcard_constraints(config['pybids_inputs'])

# Manage default case (no flags)
run_all = False
if (not config['run_all']) and (not config['epoch']) and (not config['downsample']) \
    and (not config['filter']) and (not config['rereference']) \
    and (not config['PLI_rej']) and (not config['regions_id']) and (not config['anat']):
    run_all = True

## Input definition
if config['epoch'] or config['run_all'] or run_all:
    # Get filtered ziplist based on annotations of interest if extracting epochs
    input_ziplist, number_clips = get_inputs(inputs.path['annotations'], **inputs.zip_lists['annotations'])
    # Add clip and rec to wildcards of outputs
    out_edf_wc = copy.deepcopy(inputs.wildcards['ieeg'])
    out_edf_wc['clip'] = '{clip}'
elif config['anat']:
    input_ziplist = inputs.zip_lists['T1w']
    out_edf_wc = inputs.wildcards['T1w']
else:
    input_ziplist = inputs.zip_lists['ieeg']
    out_edf_wc = inputs.wildcards['ieeg']

# Get mapping from 7T to clinical
snsx_clinical_df = pd.read_csv(os.path.join(workflow.basedir, "..", config['clinical_snsx']), sep='\t')
mapping_clinical_to_7T = dict(zip(snsx_clinical_df['ieeg_subject'].tolist(), snsx_clinical_df['snsx_subject'].tolist()))

#---- end snakebids boilerplate -----------------------------------------------
# Add rules in order
# include: "rules/preproc_t1.smk"
# include: "rules/synthseg.smk"
# # include: "rules/fmriprep.smk"
# include: "rules/reg_t1_to_template.smk"
## Epoch extraction rule
if config['epoch'] or config['run_all'] or run_all:
    include: "rules/epoch_extraction.smk"

## Downsampling
if config['downsample'] or config['run_all'] or run_all:
    include: "rules/downsample.smk"

## Cleaning
if config['filter'] or config['run_all'] or run_all:
    include: "rules/filter.smk"

## Rereferencing
if config['rereference'] or config['run_all'] or run_all:
    include: "rules/rereference.smk"

## PLI rejection
if config['PLI_rej'] or config['run_all'] or run_all:
    include: "rules/PLI_reject.smk"

## Region identification
if config['regions_id'] or config['run_all'] or run_all:
    include: "rules/transform.smk"
    include: "rules/merge_labels.smk"
    include: "rules/identify_regions.smk"

## Merge tsv files
if config['regions_id'] or config['rereference'] or config['run_all'] or run_all:
    include: "rules/merge_tsv.smk"

# Merge json files
if config['regions_id'] or config['run_all'] or run_all:
    include: "rules/merge_json.smk"

# print(config['processes'])

# include: "rules/all_rule.smk"

### RULE ALL
# Define inputs
def define_all_inputs():
    # If regions_id is the last step
    if config['run_all'] or config['regions_id']:
        # Only use last output of the rules.merge_tsv.output.out_files, which is the one under bids
        return rules.identify_regions.output.out_edf, rules.merge_tsv.output.out_files[-1], rules.merge_json.output.out_files
    # Power line rejection is the last step and rereference is called
    elif config['PLI_rej'] and config['rereference']:
        # print('PLI')
        return rules.PLI_reject.output.out_edf, rules.merge_tsv.output.out_files[-1]
    # Else if PLI is called but not rereference 
    elif config['PLI_rej']:
        # print('PLI')
        return rules.PLI_reject.output
    # Else if rereference is the last step
    elif config['rereference']:
        print(rules.rereference.output.out_edf, rules.merge_tsv.output.out_files[-1])
        print(type(rules.rereference.output.out_edf), type(rules.merge_tsv.output.out_files[-1]))
        return rules.rereference.output.out_edf, rules.merge_tsv.output.out_files[-1]
    # Else if filtering is the last step
    elif config['filter']:
        return rules.filter_data.output
    # Else if downsampling is the last step
    elif config['downsample']:
        return rules.downsample.output
    # Else if epoch extraction is the last step
    elif config['epoch']:
        return rules.get_epoch_files.output
    elif config['anat']:
        dseg = bids(
                root='work',
                datatype="anat",
                **inputs.wildcards['T1w'],
                desc="synthsegcortparc",
                suffix="dseg.nii.gz"
                )
        tmp_file = rules.greedy_t1_to_template.output.warped_flo[-1]
        return dseg, tmp_file
    # Else: default case run_all
    else:
        # print('hola')
        return rules.identify_regions.output.out_edf, rules.merge_tsv.output.out_files[-1], rules.merge_json.output.out_files

rule all:
    input:
        expand(
            expand(
                define_all_inputs(),
                allow_missing = True,
            ),
            zip,
            **input_ziplist
        ),
    default_target: True

